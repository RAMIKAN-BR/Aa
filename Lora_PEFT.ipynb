{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRzkHKCYU9UB2QUHIe3lBa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAMIKAN-BR/Aa/blob/main/Lora_PEFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!Python3 -m pip install --upgrade pip\n",
        "!pip install accelerate -U\n",
        "!pip install peft\n",
        "!pip install datasets\n",
        "!pip install transformers [torh]\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "import torch\n",
        "import transformers\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from accelerate import Accelerator\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Inicializar o acelerador\n",
        "accelerator = Accelerator()\n",
        "\n",
        "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
        "#checkpoint = \"Ramikan-BR/tinyllama_PY-CODER-bnb-4bit-lora_4k-v2\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Mover o modelo para o dispositivo correto\n",
        "model = model.to(accelerator.device)\n",
        "\n",
        "text = \"code in python a code for train AI gpt2\"\n",
        "\n",
        "# Codificar o texto e mover os tensores para o dispositivo correto\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(accelerator.device)\n",
        "\n",
        "# Gerar a conclusão\n",
        "completion = model.generate(**inputs)\n",
        "\n",
        "# Decodificar a conclusão\n",
        "output_text = tokenizer.decode(completion[0])\n",
        "\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op-Alye1eY-y",
        "outputId": "cfe2e18c-bd3f-4427-8914-be7ebc053be2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code in python a code for train AI gpt2\n",
            "    \"\"\"\n",
            "    def __init__\n"
          ]
        }
      ]
    }
  ]
}